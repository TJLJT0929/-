
\begin{document}


\section{Spectral Ranking Inferences based on General Multiway
Comparisons*}\label{spectral-ranking-inferences-based-on-general-multiway-comparisons}

Jianqing Fan Zhipeng Lou Weichen Wang Mengxin Yu

March 4, 2024

\section{Abstract}\label{abstract}

This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a general and more realistic setup. Specifically, the comparison graph consists of hyper-edges of possible heterogeneous sizes, and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in scenarios where the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptotic distributions of the estimated preference scores, we also introduce a comprehensive framework to carry out both one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It is noteworthy that this is the first time effective two-sample rank testing methods have been proposed. Finally, we substantiate our findings via comprehensive numerical simulations and subsequently apply our developed methodologies to perform statistical inferences for statistical journals and movie rankings.

\section{1 Introduction}\label{introduction}

Rank aggregation is crucial in various applications, including web search (Dwork et al., 2001; Wang et al., 2016), primate intelligence experiments (Johnson et al., 2002), assortment optimization (Aouad et al., 2018; Chen et al., 2020), recommendation systems (Baltrunas et al., 2010; Li et al., 2019), sports ranking (Massey, 1997; Turner and Firth, 2012), education (Avery et al., 2013; Caron et al., 2014), voting (Plackett, 1975; Mattei and Walsh, 2013), and instruction tuning used in the recent popular large language model ChatGPT (Ouyang et al., 2022). Therefore, it becomes an essential problem in many fields, such as psychology, econometrics, education, operation research, statistics, machine learning, artificial intelligence, etc.

Luce (Luce, 1959) introduced the celebrated Luce's axiom of choice. Let \(p(i\mid A)\) be the probability of selecting item \(i\) over all other items in the set of alternatives \(A\). According to the axiom, when comparing two items \(i\) and \(j\) in any sets of alternatives \(A\) containing both \(i\) and \(j\), the probability of choosing \(i\) over \(j\) is unaffected by the presence of other alternatives in the set. In specific, the axiom postulates that
\[
\frac{\mathbb{P}(i\mathrm{~is~preferred~in~}A)}{\mathbb{P}(j\mathrm{~is~preferred~in~}A)} = \frac{\mathbb{P}(i\mathrm{~is~preferred~in~}\{i,j\})}{\mathbb{P}(j\mathrm{~is~preferred~in~}\{i,j\})}.
\]
This assumption gives rise to a unique parametric choice model, the Bradley-Terry-Luce (BTL) model for pairwise comparisons, and the Plackett-Luce (PL) model for \(M\)-way rankings \(M\geq 2\).

In this paper, we consider a collection of \(n\) items whose true ranking is determined by some unobserved preference scores \(\theta_{i}^{*}\) for \(i = 1,\dots ,n\). In this scenario, the BTL model assumes that an individual or a random event ranks item \(i\) over \(j\) with probability \(\mathbb{P}(\mathrm{item}~i\) is preferred over item \(j) = e^{\theta_i^*} / (e^{\theta_i^*} + e^{\theta_j^*})\). The Plackett-Luce model is an expanded version of pairwise comparison, which allows for a more comprehensive \(M\)-way full ranking, as initially described in Plackett (1975). This model takes individual preferences into account when ranking a selected subset of items with size \(M< \infty\) (among all \(n\) items), which we represent as \(i_{1}\succ \dots \succ i_{M}\). Think of this full ranking as \(M - 1\) distinct events where \(i_{1}\) is favored over the set \(i_{1},\ldots ,i_{M}\), followed by \(i_{2}\) being favored over the set \(i_{2},\ldots ,i_{M}\), and so on. The PL model calculates the probability of a full ranking \(i_{1}\succ \dots \succ i_{M}\) using the formula:
\[
\mathbb{P}(i_1\succ \dots \succ i_M) = \prod_{j = 1}^{M - 1}\left[e^{\theta_{ij}^*} / \sum_{k = j}^{M}e^{\theta_{ik}^*}\right].
\]
Next, we will give a brief introduction to the literature that has made progress on model estimation and uncertainty quantification for the BTL and the PL models over the parametric model.

\section{1.1 Related literature}\label{related-literature}

A series of papers studied model estimation or inference based on the BTL or PL models. In the case of the Bradley-Terry-Luce model, its theoretical characteristics were solidified through a minorization-maximization algorithm, as outlined by Hunter (2004). Additionally, Negahban et al.~(2012) developed an iterative rank aggregation algorithm called Rank Centrality (spectral method), which can recover the BTL model's underlying scores at an optimal \(\ell_2\)-statistical rate. Subsequently, Chen and Suh (2015) used a two-step methodology (spectral method followed by MLE) to examine the BTL model in a context where the comparison graph is based on the Erdős-Rényi model, where every item pair is assumed to have a probability \(p\) of being compared, and once a pair is connected, it will be compared for \(L\) times. Subsequently, under a similar setting with Chen and Suh (2015), Chen et al.~(2019) investigated the optimal statistical rates for recovering the underlying scores, demonstrating that regularized maximum likelihood estimation (MLE) and the spectral method are both optimal for retrieving top-\(K\) items when the conditional number is a constant. They derived \(\ell_2\)-as well as \(\ell_\infty\)-rates for the unknown underlying preference scores in their study. Furthermore, Chen et al.~(2022) extended the investigation of Chen et al.~(2019) to the partial recovery scenarios and improved the analysis to un-regularized MLE.

Expanding beyond simple pairwise comparisons, researchers also explored ranking issues through \(M\)-way comparisons, where \(M \geq 2\). The Plackett-Luce model and its variations serve as prominent examples in this line of study, as evidenced by numerous references (Plackett, 1975; Guiver and Snelson, 2009; Cheng et al., 2010; Hajek et al., 2014; Maystre and Grossglauser, 2015; Szorenyi et al., 2015; Jang et al., 2018). In particular, Jang et al.~(2018) investigated the Plackett-Luce model within the context of a uniform hyper-graph, where a tuple with size \(M\) is compared with probability \(p\) and once a tuple is connected or compared in the hyper-graph, it will be compared for \(L\) times. By dividing \(M\)-way comparison data into pairs, they employ the spectral method to obtain the \(\ell_\infty\)-statistical rate for the underlying scores. Additionally, they presented a lower bound for sample complexity necessary to identify the top-\(K\) items under the Plackett-Luce model. In a more recent development, under the same model setting, Fan et al.~(2022b) enhanced the findings of Jang et al.~(2018), focusing solely on the top choice. Rather than splitting the comparison data into pairwise comparisons, they applied the Maximum Likelihood Estimation (MLE) and matched the sample complexity lower bound needed to recover the top-\(K\) items. This contrasts with Jang et al.~(2018), which requires a significantly denser comparison graph or a much larger number of comparisons for their results to hold.

The aforementioned literature primarily concentrated on the non-asymptotic statistical consistency for estimating item scores. However, the results of limiting distribution for ranking models remained largely unexplored. Only a limited number of findings on asymptotic distributions of estimated ranking scores exist under the Bradley-Terry-Luce model, whose comparison graphs are sampled from the Erdős-Rényi graph with connection probability \(p\) and each observed pair has the same number of comparisons \(L\). For example, Simons and Yao (1999) established the asymptotic normality of the BTL model's maximum likelihood estimator when all comparison pairs are entirely conducted (i.e., \(p = 1\)). Han et al.~(2020) expanded these findings for dense, but not fully connected, comparison graphs (Erdős-Rényi random graphs) where \(p \gtrsim n^{- 1 / 10}\). Recently, Liu et al.~(2022) introduced a Lagrangian debiasing approach to derive asymptotic distributions for ranking scores, accommodating sparse comparison graphs with \(p \asymp 1 / n\) but necessitating comparison times \(L \gtrsim n^2\). Furthermore, Gao et al.~(2021) employed a ``leave-two-out'' technique to determine asymptotic distributions for ranking scores, achieving optimal sample complexity and allowing \(L = \mathcal{O}(1)\) in the sparse comparison graph setting (i.e., \(p \asymp 1 / n\) up to logarithm terms). Fan et al.~(2022a) extended upon existing research by incorporating covariate information into the BTL model. By introducing an innovative proof, they presented the MLE's asymptotic variance with optimal sample complexity when \(p \asymp 1 / n\) and \(L = \mathcal{O}(1)\). In the sequel, Fan et al.~(2022b) broadened the asymptotic results of the BTL model to encompass Plackett-Luce (PL) models with \(M \geq 2\) again with optimal sample complexity. They also developed a unified framework to construct rank confidence intervals, requiring the number of comparisons \(L \gtrsim \mathrm{poly}(\log n)\). Moreover, recently, Han and Xu (2023) further extended the settings in Fan et al.~(2022b) by investigating the asymptotic distribution of the MLE, where the comparisons are generated from a mixture of Erdos-Renyi graphs with different sizes or a hypergraph stochastic block model.

Finally, we discuss related literature of an important application of our framework: assortment optimization (Talluri and Van Ryzin, 2004; Rusmevichientong and Topaloglu, 2012; Vulcano et al., 2012; Davis et al., 2014; Zhang et al., 2020; Chen et al., 2020, 2023; Shen et al., 2023) which is of great importance in revenue management. Specifically, in their settings, each product (including the no-purchase alternative) is also associated with an unknown customer preference score, which can characterize the customers' choice behavior over a set of offered products. Based on the consistently estimated preference scores and the available profit information of each product, various efficient algorithms have been proposed to determine the optimal assortment under different kinds of practical constraints (Talluri and Van Ryzin, 2004; Rusmevichientong et al., 2010; Gallego and Topaloglu, 2014; Sumida et al., 2021). Moreover, uncertainty quantification of the estimated preference scores also enables statistical inference on the properties of the optimal assortment (Shen et al., 2023).

\section{1.2 Motivations and Contributions}\label{motivations-and-contributions}

In this section, we discuss our motivation and problem settings and compare our results with previous literature in different aspects, namely the comparison graph, the connection between the spectral method and MLE, and ranking inferences.

\section{1.2.1 Comparison graph}\label{comparison-graph}

Previous studies on parametric ranking models mostly require comparison graphs derived from a specific random graph. For example, in the endeavor of understand multiway comparisons (e.g.~Jang et al.~(2016); Fan et al.~(2022b)), it is typically assumed that the comparisons are generated explicitly from a homogeneous graph with a known distribution. This assumption may pose a challenge in certain contexts. Although practical applications with homogeneous comparisons exist, it is sometimes unrealistic to presume that all comparisons are generated from a known homogeneous distribution. In fact, there are more cases where we see heterogeneous comparisons, where some are compared more often than others and the comparison graph generation process is unknown. We will present Example 1.1 as our motivation.

Example 1.1. There is a sequence of customers buying goods. For the \(l\)-th customer, according to her preference, her reviewed products are \(A_{l}\) (a choice set), and she finally chose product \(c_{l}\in A_{l}\) (her top choice). Then the total datasets presented to us is \(\{(A_l,c_l)\}_{l = 1}^D\). If all choice sets are presented to the customers with the same probability and the reviewed number of items are of the same size (such as pairwise comparisons), we say the comparison graph is homogeneous. But if some choice sets, possibly with different sizes, are presented more often to the customers or are chosen arbitrarily based on the customers' preference profiles, then this heterogeneous comparison graph cannot be well approximated by a given random graph model. That is, the comparisons may not follow, say, the BTL or PL models with Erdos-Renyi types of uniform graphs.

The heterogeneous comparison scheme in Example 1.1 above is applicable across a wide range of practical scenarios. For instance, it covers the typical setup of the assortment optimization,
\pandocbounded{\includegraphics[keepaspectratio]{images/166cf6f6b99e4beefb26f15500d7fb45e7a5c1bf6743c6b0bcc83e3bdcb260e6.jpg}}
wherein the no-purchase alternative is also included in the choice set \(A_{l}\). We give a toy example with 5 products in Figure 1, where sizes of \(A_{l}\) are among \(\{2,3,4\}\). Due to the heterogeneity, it is unrealistic to assume \(A_{l}\) is of the same size or sampled from an explicit random graph. However, most of the previous works focused on statistical properties under certain ad-hoc random graphs, most commonly the Erdős-Rényi type of uniform graphs, e.g., Chen and Suh (2015); Chen et al.~(2019); Jang et al.~(2016); Gao et al.~(2021); Liu et al.~(2022); Fan et al.~(2022b); Han and Xu (2023). One interesting piece of research that indeed worked with the fixed comparison graph is Shah et al.~(2015), which explored the optimality of MLE in \(\ell_{2}\) estimation with pairwise comparisons. Following this work, Li et al.~(2022) further discussed the optimality of MLE in \(\ell_{\infty}\) error. Still, little has been known about the inference results for general fixed comparison graphs.

In this paper, we focus on the setting of a general fixed comparison graph, where we circumvent the modeling for the complicated graph-generating process. Specifically, we study statistical estimation and uncertainty quantification of the preference scores over a general, heterogeneous choice set via the spectral method. In addition, we also study the theoretical performance of the spectral method when we do have a homogeneous random comparison graph, and compare the results. Our results require slightly stronger conditions to handle fixed graph settings since we need to make sure each item has enough information to be ranked.

For the general setting, we denote the choice preference for the \(l\)-th comparison as \((c_{l},A_{l})\), wherein \(A_{l}\) signifies the set of choices with heterogeneous size, which can be either fixed or random, and \(c_{l}\in A_{l}\) represents the most preferred item in \(A_{l}\). Hence, in the \(l\)-th comparison, we understand that \(c_{l}\) outranks all other elements within \(A_{l}\). As such, our broadest comparison data is symbolized as \(\mathcal{D} = \{l|(c_{l},A_{l})\}\). The associated collection of choice sets is denoted as \(\mathcal{G} = \{A_{l}|l\in \mathcal{D}\}\). This framework also contains the Plackett-Luce model as a special case, if we treat the PL model as \(M - 1\) selections over \(M - 1\) choice sets. Under mild conditions, we manage to obtain optimal statistical rates for our spectral estimator conditional on the comparison graphs and specify the explicit form of its asymptotic distribution. This gives an efficient solution when one encounters heterogeneous comparison graphs. In addition, since the graph is fixed or conditioned upon, it is not necessary for us to repeat each comparison for \(L\geq 1\) times. We can even accommodate situations where a choice set is chosen and compared for just a single time, which is true in many practical applications.

\section{1.2.2 Connection between the spectral estimator and the MLE}\label{connection-between-the-spectral-estimator-and-the-mle}

Our general setting, as introduced in Section 1.2.1, encompasses homogeneous random comparison graphs as a particular instance. The bulk of prior research has centered around the evaluation of the Maximum Likelihood Estimator (MLE) or the spectral method when applied to homogeneous comparisons (Chen and Suh, 2015; Chen et al., 2019, 2022; Fan et al., 2022a,b). Both are effective methods for examining ranking issues within the context of the BTL or PL model. Hence, an interesting question arises: What is the relationship between the MLE and the spectral method?

A handful of studies have offered insights into this question. For instance, Maystre and Gross-glauser (2015) identified a link between the MLE and spectral method in multiway comparisons, where the spectral estimator aligns with the MLE through the iterative updating of specific weighting functions in constructing the spectral estimator. This connection was only limited to the first order in the sense that the paper only concerns the convergence property. Furthermore, Gao et al.~(2021) demonstrated that the asymptotic variance of the spectral method exceeds that of the MLE in pairwise comparisons using the BTL model. However, this discrepancy arises from their choice of a suboptimal weighting function for the spectral estimator.

In our paper, we leverage the homogeneous random comparison graph case (as it is the most popularly studied setting in many previous articles) to illustrate that by employing certain optimally estimated information weighting functions, the asymptotic variance of the spectral estimator matches that of the MLE with multiway comparisons in the PL model. Therefore, the MLE could be considered a ``two-step'' spectral method, where in the first step we consistently estimate the unobserved preference scores, and in the second step we use the proper weighting in the spectral method to achieve an efficient estimator. It is also noteworthy that we achieve the optimal sample complexity over the sparsest sampling graph up to logarithmic terms.

\section{1.2.3 Ranking inferences: one sample vs two samples}\label{ranking-inferences-one-sample-vs-two-samples}

As another contribution, we also study several rank-related inference problems. We have the following motivating example:

Example 1.2. First, we consider the one-sample inference problem. Consider a group of candidate items \(\{1,\dots ,n\}\) and one observed dataset on their comparisons; we are interested in
\begin{itemize}
\tightlist
\item
  Building the confidence intervals for the ranks of certain items \(\{r_1,\dots ,r_m\}\) of our interest.
\item
  Testing whether a given item \(m\) is in the top-\(K\) set, which includes \(K\) best items.
\end{itemize}
Second, we consider the two-sample inference problem. For two groups of datasets of the same list of items \(\{1,\dots ,n\}\), we are interested in
\begin{itemize}
\tightlist
\item
  Testing whether the rank of a certain item \(m\) is preserved across these two samples (e.g.~groups or time periods).
\item
  Testing whether the top-\(K\) sets have changed or not.
\end{itemize}

Rankings are ubiquitous in real-world applications, for instance, in the evaluation of universities, sports teams, or web pages. However, most of these rankings provide only first-order information, presenting the results without offering any measure of uncertainty. For example, under the BTL model, when two items have equivalent underlying scores, there's a \(50\%\) probability of one being ranked higher than the other. Thus, rankings between these two items could be unreliable due to their indistinguishable underlying scores, emphasizing the necessity for confidence intervals in rankings.

Given these critical considerations, our study offers a comprehensive framework that efficiently solves the problems outlined in Example 1.2 over heterogeneous comparison graphs. Additionally, our approach enhances the sample complexity of several previous works. For instance, when restricting our general framework to homogeneous random comparison graphs, regarding the one-sample inference, Liu et al.~(2022) required \(L \gtrsim n^2\) to carry out the statistical inference, while Fan et al.~(2022b) further improved this requirement to \(L \gtrsim \mathrm{poly}(\log n)\). In our paper, our framework can allow \(L = \mathcal{O}(1)\) and even \(L = 1\). Furthermore, two-sample ranking inference, which can be widely applied in real-world scenarios like policy evaluation, treatment effect comparison, change point detection, etc., has not been previously studied. Our paper also introduces a general framework for studying the two-sample ranking inference problems, again offering optimal sample complexity.

\section{1.2.4 Theoretical contributions}\label{theoretical-contributions}

We build up our theoretical analyses based on some previously developed techniques from Gao et al.~(2021) and Chen et al.~(2019). However, our proofs have the following novelty: In those two papers and other papers with a random comparison graph, graph randomness and ranking outcome randomness are typically intertwined in the analysis. We will separate them and reveal the proper quantities of interest to summarize the information in a fixed comparison graph. We study the connection between these quantities and the spectral ranking performance, and provide sufficient conditions under the fixed graph for valid ranking inference. This theoretical attempt has not previously been seen in the literature. In addition, all our analyses allow varying comparison sizes and an arbitrary number of repetitions of each comparison set. This significantly broadens the applicability of our proposed methodology, as in practice, a lot of ranking problems contain non-repeating comparisons of different numbers of items. We also work on the theory when we also have graph randomness. We realize that the homogeneity of sampling each comparison tuple can lead to more relaxed assumptions. With more relaxed conditions, we clearly show where and how we can achieve an improved performance guarantee (see the assumption and proof of Theorem 4.4). This part highlights the difference between fixed and random graphs and provides more theoretical insights into the role of graph randomness in spectral ranking.

\section{1.3 Roadmap and notations}\label{roadmap-and-notations}

In Section 2, we set up the model and introduce the spectral ranking method. Section 3 is dedicated to the examination of the asymptotic distribution of the spectral estimator, based on fixed comparison graphs and random graphs with the PL model, respectively. Within the same section, we also introduce the framework designed for the construction of rank confidence intervals and rank testing statistics for both one-sample and two-sample analysis. Section 4 details the theoretical guarantees for all proposed methodologies. Sections 5 and 6 contain comprehensive numerical studies to verify theoretical results and two real data examples to illustrate the usefulness of our ranking inference methods. Finally, we conclude the paper with some discussions in Section 7. All the proofs are deferred to the appendix.

Throughout this work, we use \([n]\) to denote the index set \(\{1,2,\dots ,n\}\). For any given vector \(\mathbf{x} \in \mathbb{R}^n\) and \(q \geq 0\), we use \(\| \mathbf{x}\| _q = (\sum_{i = 1}^{n} |x_i|^q)^{1 / q}\) to represent the vector \(\ell_q\) norm. For any given matrix \(\mathbf{X} \in \mathbb{R}^{d_1 \times d_2}\), we use \(\| \cdot \|\) to denote the spectral norm of \(\mathbf{X}\) and write \(\mathbf{X} \succcurlyeq 0\) or \(\mathbf{X} \preccurlyeq 0\) if \(\mathbf{X}\) or \(- \mathbf{X}\) is positive semidefinite. For event \(A\), \(1(A)\) denotes an indicator which equals 1 if \(A\) is true and 0 otherwise. For two positive sequences \(\{a_n\}_{n \geq 1}\), \(\{b_n\}_{n \geq 1}\), we write \(a_n = \mathcal{O}(b_n)\) or \(a_{n} \lesssim b_{n}\) if there exists a positive constant \(C\) such that \(a_{n} / b_{n} \leq C\) and we write \(a_{n} = o(b_{n})\) if \(a_{n} / b_{n} \to 0\). In addition, \(\mathcal{O}_{p}(\cdot)\) and \(o_{p}(\cdot)\) share similar meanings as \(\mathcal{O}(\cdot)\) and \(o(\cdot)\), respectively, but these relations hold asymptotically with probability tending to 1. Similarly we have \(a_{n} = \Omega (b_{n})\) or \(a_{n} \gtrsim b_{n}\) if \(a_{n} / b_{n} \geq c\) with some constant \(c > 0\). We use \(a_{n} = \Theta (b_{n})\) or \(a_{n} \asymp b_{n}\) if \(a_{n} = \mathcal{O}(b_{n})\) and \(a_{n} = \Omega (b_{n})\). For two random variables \(A_{n}, B_{n}\), if we write \(A_{n} \approx B_{n}\), it holds that \(A_{n} - B_{n} = o(1)\) with probability goes to 1. Given \(n\) items, we use \(\theta_{i}^{*}\) to indicate the underlying preference score of the \(i\)-th item. Define \(r: [n] \to [n]\) as the rank operator on the \(n\) items, which maps each item to its population rank based on the preference scores. We write the rank of the \(i\)-th item as \(r_{i}\) or \(r(i)\). By default, we consider ranking from the largest score to the smallest score.

\section{2 Multiway Comparison Model and Spectral Ranking}\label{multiway-comparison-model-and-spectral-ranking}

We first introduce a general discrete choice model, which encompasses the classical Plackett-Luce model as well as fixed comparison graph scenario.

\section{2.1 Discrete choice model}\label{discrete-choice-model}

We assume there are \(n\) items to be ranked. According to Luce's choice axiom (Luce, 1959), the preference scores of a given group of \(n\) items can be parameterized as a vector \((\theta_{1}^{*}, \ldots , \theta_{n}^{*})^{\top}\) such that \(\mathbb{P}(i\) wins among \(A) = e^{\theta_{i}^{*}} / (\sum_{k \in A} e^{\theta_{k}^{*}})\) for any choice set \(A\) and item \(i \in A\). Since the parameters are only identifiable up to a location shift, without loss of generality, we assume \(\sum_{i = 1}^{n} \theta_{i}^{*} = 0\) for identification. We consider the general comparison model, where we are given a collection of comparisons and outcomes \(\{(c_{l}, A_{l})\}_{l \in \mathcal{D}}\). Here \(c_{l}\) denotes the selected item over the choice set \(A_{l}\) with probability \(e^{\theta_{c_{l}}^{*}} / (\sum_{k \in A_{l}} e^{\theta_{k}^{*}})\).

Remark 2.1. This general comparison model contains many well-known special cases.
\begin{itemize}
\item
  For the Bradley-Terry-Luce (BTL) model, it is easy to set \(A_{l}\) as the pair being compared every time. If each pair is compared for \(L\) times independently, we just need to write the outcomes as \((c_{l}, A_{l})\) and re-index \(l\).
\item
  For the Plackett-Luce (PL) model, we have obtained the full ranking of a choice set \(B = \{i_{1}, \dots , i_{B}\}\). The probability of observing a certain ranking becomes
\end{itemize}
\[
\begin{array}{r l} & {\mathbb{P}(i_{1} > i_{2} > \dots >i_{B}) = \mathbb{P}(i_{1}\mathrm{~wins~among~}C_{1}\mid C_{1} = B)\cdot \mathbb{P}(i_{2}\mathrm{~wins~among~}C_{2}\mid C_{2} = B\{-i_{1}\})\cdot[...]}
\end{array}
\]
where \(C_{i}, i \geq 1\) is the \(i\)-th comparison set and \(B\{- i_{1}, \dots , - i_{M}\}\) denotes the set of remained items after removing \(\{i_{1}, \dots , i_{M}\}\). These comparison results can also be decomposed into the comparisons:
\[
\{(i_{1}, B), (i_{2}, B\{-i_{1}\}), \dots , (i_{B - 1}, B\{-i_{1}, \dots , -i_{B - 2}\})\} .
\]
\begin{itemize}
\tightlist
\item
  With fixed comparison graphs, \(\{A_{l},l\in \mathcal{D}\}\) are given and hence have no randomness, so the comparison results in \(c_{l}\) are assumed independent. In contrast, with a random comparison graph, such as in the PL model, \(A_{l}\) may be dependent. For instance, \((\theta_{i_{1}},B)\) and \((\theta_{i_{2}},B\{-i_{1}\})\) are dependent as \(B\{-i_{1}\}\) depends on the winner of the first comparison \(i_{1}\). Therefore, we have to explicitly lay out the random process assumption for comparison generation in order to study the theoretical properties in a case-by-case manner.
\end{itemize}

Recall that the general comparison data is denoted as \(\{(c_{l},A_{l})\}_{l\in \mathcal{D}}\). The corresponding collection of choice sets is \(\mathcal{G} = \{A_{l}|l\in \mathcal{D}\}\). When we only have pairwise comparisons, \(|A_{l}| = 2\) and \(\mathcal{G}\) represents the set of all edges we have compared. But in a general setting, \(A_{l}\) can have different sizes and we denote \(M = \max_{l\in \mathcal{D}}|A_{l}|< \infty\) as the maximal size of the comparison hyper-graph edge. Also if we have \(L\) independent comparisons of the same comparison hyper-edge \(A_{l}\), we use different \(l\) to indicate the comparison. So in \(\mathcal{G}\), the hyper-edge \(A_{l}\) may be compared for multiple times with different outcomes \(c_{l}\).

Throughout this paper, we consider using the spectral method on the preference data based on multiway comparisons. We will first focus on the fixed comparison graph and then consider commonly-used random comparison graph structures. Notice that no matter whether the comparison graph \(\mathcal{G}\) is fixed or random, our spectral ranking methodology will be conditional on \(\mathcal{G}\), which is observed in practice. The underlying model for generating \(\mathcal{G}\) can be very general: it can be given, or random based on the Erdős-Rényi random graph with the same probability \(p\), or more generally induced from some other comparison rules, which could even cause some \(A_{l}\) dependent. For example, if we view each comparison of the PL model as \(M - 1\) pairwise comparisons involving top 1 vs top 2, top 2 vs top 3, \ldots, top \(M - 1\) vs top \(M\). Then the resulting comparison data, denoted as \((c_{l} - i_{k},A_{l} = \{i_{k},i_{k + 1}\})\) for \(k = 1,\ldots ,M - 1\), are dependent (even the definition of \(A_{l}\) depends on the complete comparison result).

\section{2.2 Spectral ranking}\label{spectral-ranking}

In the spectral method, we formally define a Markov chain, denoted as \(M = (S,P)\). Here, \(S\) signifies the collection of \(n\) states corresponding to the \(n\) items to be compared, represented as vertices of a directed comparison graph. And \(P\) constitutes the transition matrix defined below. This matrix oversees transitions amongst the states by representing whether any two particular states within \(S\) are capable of being connected via non-zero transition probabilities.

Define two index sets \(\mathcal{W}_{j},\mathcal{L}_{i}\) for comparisons, with \(j\) as the winner and \(i\) as the loser:
\[
\mathcal{W}_{j} = \{l\in \mathcal{D}|j\in A_{l},c_{l} = j\} ,\qquad \mathcal{L}_{i} = \{l\in \mathcal{D}|i\in A_{l},c_{l}\neq i\} .
\]
So their intersection for \(i\neq j\) gives all situations where \(i,j\) are compared and \(j\) wins, i.e., \(\mathcal{W}_{j}\cap \mathcal{L}_{i} = \{l\in \mathcal{D}|i,j\in A_{l},c_{l} = j\}\). Define the transition matrix \(P\) with transition probability
\[
P_{ij} = \left\{ \begin{array}{ll}\frac{1}{d}\sum_{l\in \mathcal{W}_{j}}\mathcal{L}_{i}\frac{1}{f(A_{l})}, & \text{if} i\neq j, \\ 1 - \sum_{k:k\neq i}P_{ik}, & \text{if} i = j. \end{array} \right.
\]
Here, \(d\) is chosen to be large enough so that the diagonal element is non-negative, but not too large to give enough transition probability. When the comparison graph is random, we choose \(d\) to make non-negative diagonal elements with probability approaching 1 by studying the concentration inequality for \(\sum_{k:k\neq i}P_{i k}\). Here, \(f(A_{l}) > 0\) is a weighting function to encode the total information in the \(l\)-th comparison. A natural choice is \(f(A_{l}) = |A_{l}|\) giving more weight to hyper-edges with a smaller number of compared items. We will discuss later the optimal choice of \(f(\cdot)\). When \(i\neq j\), \(P_{i j}\) can also be written as
\[
P_{i j} = \frac{1}{d}\sum_{l\in \mathcal{D}}1(i,j\in A_{l})1(c_{l} = j)\frac{1}{f(A_{l})}
\]
Conditioning on \(\mathcal{G}\), the population transition is
\[
P_{i j}^{*} = E[P_{i j}|\mathcal{G}] = \left\{ \begin{array}{l l}{\frac{1}{d}\sum_{l\in \mathcal{D}}1(i,j\in A_{l})\frac{e^{\theta_{j}^{*}}}{\sum_{u\in A_{l}}e^{\theta_{u}^{*}}}\frac{1}{f(A_{l})},} & [...]
\end{array} \right.
\]
Let
\[
\pi^{*} = (e^{\theta_{1}^{*}},\ldots ,e^{\theta_{n}^{*}}) / \sum_{k = 1}^{n}e^{\theta_{k}^{*}}.
\]
Note that both \(\sum_{u\in A_{l}}e^{\theta_{u}^{*}}\) and \(f(A_{l})\) in the denominator are symmetric with respect to \(\theta_{i}^{*},\theta_{j}^{*}\) as long as both \(i,j\) belong to \(A_{l}\). So we have \(P_{i j}^{*}\pi_{i}^{*} = P_{i j}^{*}\pi_{j}^{*}\). This is the so-called detailed balance that leads to \(\pi^{*}\) being the stationary measure of the Markov chain with the above population transition for any \(f(\cdot)\). That is \(\pi^{*}^{\top}P^{*} = \pi^{*}^{\top}\), namely, \(\pi^{*}\) is the top-left eigenvector of \(P^{*}\).

The spectral method estimates \(\pi^{*}\) by using the stationary measure \(\widehat{\pi}\) of the empirical transition \(P\), namely,
\[
\widehat{\pi}^{\top}P = \widehat{\pi}^{\top}.
\]
Note that if we consider the directed graph induced by \(P\) to be strongly connected, this implies that the Markov chain it generates will be ergodic, which ensures the existence of a unique stationary distribution \(\widehat{\pi}\) as defined above. Consider the toy example in Figure 1, if we naively choose \(f(\cdot) = 1\) as a constant weighting function, it is not hard to count the times that \(j\) beats \(i\) and fill the value into the transition matrix \(P_{i j}\) (divided by \(d\)). In the right panel, the transition probabilities are calculated with \(d = 6\) to guarantee the self-loop transition happens with a positive probability. For this \(P\), the stationary distribution is \(\widehat{\pi} = (0.199,0.531,0.796,0.199,0.066)^{\top}\), meaning that the estimated ranking of preference scores of the 5 products are \(3\succ 2\succ 1 = 4\succ 5\).

Finally given the identifiability condition of \(1^{\top}\theta^{*} = 0\), we can estimate \(\theta_{i}^{*}\) by
\[
\widehat{\theta}_{i}\coloneqq \log \widehat{\pi}_{i} - \frac{1}{n}\sum_{k = 1}^{n}\log \widehat{\pi}_{k}. \tag{2.1}
\]
It is worth mentioning that the spectral estimator is easier to compute in practice, by only requiring one-step eigen-decomposition. Indeed, we need only the eigenvector that corresponds to the largest eigenvalue, which can even be computed very fast by the power method. In comparison, the MLE is typically computationally heavier in terms of data storage and step size determination during the implementation of the gradient descent algorithm.

\section{3 Ranking Inference Methodology}\label{ranking-inference-methodology}

In this section, we study the inference methodology for the spectral estimator for the underlying scores \(\{\theta_{i}^{*}\}_{i\in [n]}\) of \(n\) items. To be specific, we need to establish the statistical convergence rates and asymptotic normality for \(\widetilde{\theta_{i}}\).

\section{3.1 Uncertainty quantification with fixed comparison graph}\label{uncertainty-quantification-with-fixed-comparison-graph}

For the estimation of \(\pi^{*}\), we use the following two approximations, which we will justify later to be accurate enough so as not to affect the asymptotic variance. Let us first focus on our intuition. Firstly, we have
\[
\widehat{\pi}_{i} = \frac{\sum_{j:j\neq i}P_{ji}\widehat{\pi}_{j}}{\sum_{j:j\neq i}P_{ij}}\approx \frac{\sum_{j:j\neq i}P_{ji}\pi_{j}^{*}}{\sum_{j:j\neq i}P_{ij}} \eqqcolon \bar{\pi}_{i}.
\]
Equivalently,
\[
\frac{\widehat{\pi}_{i} - \pi_{i}^{*}}{\pi_{i}^{*}}\approx \frac{\bar{\pi}_{i} - \pi_{i}^{*}}{\pi_{i}^{*}} = \frac{\sum_{j:j\neq i}(P_{ji}\pi_{j}^{*} - P_{ij}\pi_{i}^{*})}{\pi_{i}^{*}\sum_{j:j\neq i}P_{ij}}.
\]
Secondly, the denominator above can be approximated by its expected value so that (3.1) can further be approximated as
\[
J_{i}^{*}\coloneqq \frac{\sum_{j:j\neq i}(P_{ji}e^{\theta_{j}^{*}} - P_{ij}e^{\theta_{i}^{*}})}{\sum_{j:j\neq i}E[P_{ij}|\mathcal{G}]e^{\theta_{i}^{*}}}, \tag{3.2}
\]
by using \(\pi_{i}^{*}\propto e^{\theta_{i}^{*}}\). We will rigorously argue that the asymptotic distributions of \(\frac{\widehat{\pi}_{i} - \pi_{i}^{*}}{\pi_{i}^{*}}\) and \(J_{i}^{*}\) are identical. For now, let us look at the asymptotic distribution of \(J_{i}^{*}\). Obviously, it is mean zero due to the detailed balance: \(E[P_{ji}|\mathcal{G}]\pi_{j}^{*} = E[P_{ij}|\mathcal{G}]\pi_{i}^{*}\). The denominator of \(J_{i}^{*}\) is a constant and can be explicitly written out as follows:
\[
\tau_{i}(\theta^{*})\coloneqq \sum_{j:j\neq i}E[P_{ij}|\mathcal{G}]e^{\theta_{i}^{*}} = \frac{1}{d}\sum_{l\in \mathcal{D}}1(i\in A_{l})\left(1 - \frac{e^{\theta_{i}^{*}}}{\sum_{u\in A_{l}}e^{\theta_{u}^{*}}}\right)\frac{e^{\theta_{i}^{*}}}{f(A_{l})}.
\]
Thus, \(J_{i}^{*}\) can be expressed as
\[
J_{i}^{*} = \frac{1}{d\tau_{i}}\sum_{l\in \mathcal{D}}\frac{1(i\in A_{l})}{f(A_{l})}\left(1(c_{l} = i)\sum_{u\in A_{l},u\neq i}e^{\theta_{u}^{*}} - e^{\theta_{i}^{*}}\sum_{u\in A_{l},u\neq i}1(c_{l} = u)\right),
\]
where \(\tau_{i}\) is short for \(\tau_{i}(\theta^{*})\). Since each \((c_{l},A_{l})\) is independent in the fixed graph setting (see Remark 2.1 for discussions), the variance of \(J_{i}^{*}\) is
\[
\begin{array}{r l} & {\mathrm{Var}(J_{i}^{*}|\mathcal{G}) = \frac{1}{d^{2}\tau_{i}^{2}}\sum_{l\in \mathcal{D}}\frac{1(i\in A_{l})}{f^{2}(A_{l})}\cdot \mathrm{Var}\bigg(1(c_{l} = i)\sum_{u\in A_{l},u\neq i}e^{\theta_{u}^{*}} - e^{\theta_{i}^{*}}\sum_{u\in A_{l},u\neq i}1(c_{l} = u)\bigg|\mathcal{G}\bigg).}
\end{array}
\]
A few important comments are in order. Firstly, the function \(f\) achieves the minimal variance in (3.5) when \(\begin{array}{r}{f(A_{l})\propto \sum_{u\in A_{l}}e^{\theta_{u}^{*}}} \end{array}\) due to simply applying the Cauchy-Schwarz inequality. Actually, Maystre and Grossglauser (2015) showed that when \(\begin{array}{r}f(A_l) = \sum_{u\in A_l}e^{\theta_u^*} \end{array}\), spectral method estimator converges to the MLE up to the first order. Secondly, under the situation of pairwise comparison in the BTL model, each \((c_l,A_l)\) is independent and we assume in \(\mathcal{D}\) each pair \((i,j)\) is either compared for \(L\) times (denoted as \(\widetilde{A}_{ij} = 1\)) or never compared (denoted as \(\widetilde{A}_{ij} = 0\)). Further assuming \(f(A_{l}) = |A_{l}| = 2\), we have
\[
\operatorname {Var}(J_i^* |\mathcal{G}) = \frac{1}{L}\bigg(\sum_{j:j\neq i}\widetilde{A}_{ij}e^{\theta_i^*}e^{\theta_j^*}\bigg)\bigg / \bigg[\sum_{j:j\neq i}\widetilde{A}_{ij}\frac{e^{\theta_i^*}e^{\theta_j^*}}{(e^{\theta_i^*} + e^{\theta_j^*})}\bigg]^2.
\]
This exactly matches with Proposition 4.2 of Gao et al.~(2021). In addition, if we choose \(f(A_{l}) = \sum_{u\in A_l}e^{\theta_u^*}\), we get the most efficient variance just like the MLE variance given in Proposition 4.1 of Gao et al.~(2021), which is
\[
\operatorname {Var}(J_i^* |\mathcal{G}) = \left(L\cdot \sum_{j:j\neq i}\widetilde{A}_{ij}\frac{e^{\theta_i^*}e^{\theta_j^*}}{(e^{\theta_i^*} + e^{\theta_j^*})^2}\right)^{-1}.
\]
With the above discussion and computation, after some additional derivations, we come to the conclusion that \(\widetilde{\theta}_{i} - \theta_{i}^{*}\) has the same asymptotic distribution as \(\frac{\widetilde{\pi}_{i} - \pi_{i}^{*}}{\pi_{i}^{*}}\) and \(J_{i}^{*}\). Therefore,
\[
\operatorname {Var}(J_i^* |\mathcal{G})^{-1 / 2}(\widetilde{\theta}_i - \theta_i^*)\Rightarrow N(0,1),
\]
for all \(i\in [n]\). Based on this result, we can make inference for \(\widetilde{\theta}_{i}\) and additionally the rank of item \(i\) (see Section 3.3). The rigorous derivations for this conclusion will be provided in Section 4.

\section{3.2 Uncertainty quantification for the PL model}\label{uncertainty-quantification-for-the-pl-model}

In this section, we consider the case of a random comparison graph, which could potentially lead to dependent \((c_l,A_l)\), unlike the case of the fixed graph in the previous section. Note that since the random comparison graph generation can be arbitrary, we cannot work with each case. As an illustrating example, we consider the classical PL model from the Erdos-Renyi graph here for two reasons. Firstly, this is the most popularly studied random comparison model in the ranking literature (Chen and Suh, 2015; Chen et al., 2022; Han et al., 2020; Liu et al., 2022; Gao et al., 2021; Fan et al., 2022a,b). Secondly, we can use the model to verify the uncertainty quantification of the spectral method and compare it with that of the MLE. It turns out the model is good enough to give us new insights. To further simplify the discussion and presentation, we will only focus on \(M = 3\) in the PL model. Results for general \(M\) can be similarly derived with the same conclusion.

With the PL model, we can write down the specific variance of \(J_{i}^{*}\). Consider the most natural way to encode a 3-way comparison. Say one ranks \((i,j,k)\) as \(i\vdash j\vdash k\) where \(a\vdash b\) means \(a\) is better than \(b\). Motivated by the likelihood function, which multiplies the probability of selecting \(i\) as the best one from \(\{i,j,k\}\) and the probability of selecting \(j\) next from \(\{j,k\}\), we break this complete 3-way comparison into two dependent comparison data: \((i,\{i,j,k\})\) and \((j,\{j,k\})\). We call this multi-level breaking, where in the first level comparison of all three items, \(i\) is preferred, and in the second level comparison of the remaining items, \(j\) is preferred. By doing this, we can naturally link and compare results with the MLE estimator. Azari Soufiani et al.~(2013) also proposed other ways of breaking an \(M\)-way comparison into pairwise comparisons, but different breaking methods will lead to different dependent structures, which we do not intend to analyze one by one in this work. So in the sequel, we only consider multi-way breaking, motivated by the likelihood function, and leave the study of other possible breaking methods to the future. We use \(\widetilde{A}_{ijk} = 1\) or \(0\) to denote whether \((i,j,k)\) has been compared for \(L\) times or is never compared.

Let us work on the multi-level breaking. Now the key difference is that the induced comparison graph \(\mathcal{G}\) cannot be treated as fixed. Instead, we condition on \(\widetilde{\mathcal{G}} = \{\widetilde{A}_{ijk}\}\). Similar to (3.2), we have
\[
\frac{\bar{\pi}_{i} - \pi_{i}^{*}}{\pi_{i}^{*}} = \frac{\sum_{j:j\neq i}P_{ji}\pi_{j}^{*} - P_{ij}\pi_{i}^{*}}{\pi_{i}^{*}\sum_{j:j\neq i}P_{ij}}\approx \frac{\sum_{j:j\neq i}P_{ji}e^{\theta_{j}^{*}} - P_{ij}e^{\theta_{i}^{*}}}{\sum_{j:j\neq i}E[P_{ij}|\widetilde{\mathcal{G}}]e^{\theta_{i}^{*}}}.
\]
In the case of the random comparison graph, i.e.~conditioning on \(\widetilde{\mathcal{G}}\), we obtain
\[
P_{ij} = \frac{1}{d}\sum_{\ell = 1}^{L}\sum_{k:k\neq i,j}\widetilde{A}_{ijk}Z_{ijk}^{\ell}, \tag{3.7}
\]
where \(Z_{ijk}^{\ell} = 1(y_{k\succ j \succ i}^{(\ell)} = 1) / f(\{i,j\}) + 1(y_{j\succ k \succ i}^{(\ell)} = 1) / f(\{i,j,k\}) + 1(y_{j\succ k\succ i}^{(\ell)} = 1) / f(\{i,j,k\})\). Here \(y_{i_1\succ i_2\succ i_3}^{(\ell)}\) is a binary variable which equals to \(1\) when the event \(i_1\succ i_2\succ i_3\) holds under the \(\ell\)-th comparison among items \(\{i_1,i_2,i_3\}\). Essentially, we need to collect all terms induced from the same comparison into one term \(Z_{ijk}^{\ell}\) so that the summation is always over independent terms.

We lightly abuse the notation of \(J_{i}^{*}\) although here the expectation is conditioning on \(\widetilde{\mathcal{G}}\) instead of \(\mathcal{G}\) used in the fixed graph case. Note that
\[
E[Z_{ijk}^{\ell}|\widetilde{\mathcal{G}} ] = \frac{e^{\theta_{k}^{*}}e^{\theta_{j}^{*}}}{(e^{\theta_{i}^{*}} + e^{\theta_{j}^{*}} + e^{\theta_{k}^{*}})(e^{\theta_{i}^{*}} + e^{\theta_{j}^{*}})f(\{i,j\})}+\frac{e^{\theta_{j}^{*}}e^{\theta_{k}^{*}}}{(e^{\theta_{i}^{*}} + e^{\theta_{j}^{*}} + e^{\theta_{k}^{*}})(e^{\theta_{i}^{*}} + e^{\theta_{k}^{*}})f(\{i,k\})}.
\]
Using \(\sum_{j\neq k}a_{ijk} = \sum_{j< k}(a_{ijk} + a_{ikj})\), the denominator of \(J_{i}^{*}\) can be expressed as
\[
\begin{array}{r l} & {\tau_{i}^{\diamond}(\theta^{*})\coloneqq \sum_{j:j\neq i}E[P_{i j}|\widetilde{\mathcal{G}} ]e^{\theta_{i}^{*}} = \frac{L}{d}\sum_{j< k:j,k\neq i}\widetilde{A}_{i j k}e^{\theta_{i}^{*}}\bigg(\frac{e^{\theta_{j}^{*}}}{f(\{i,j\})} + \frac{e^{\theta_{k}^{*}}}{f(\{i,k\})}\bigg).}
\end{array}
\]
Hence, the expression of \(J_{i}^{*}\) is given as follows:
\[
\begin{array}{r l} & {J_{i}^{*} = \frac{1}{\tau_{i}^{\diamond}}\Big(\sum_{j:j\neq i}P_{j i}e^{\theta_{j}^{*}} - P_{i j}e^{\theta_{i}^{*}}\Big) = \frac{1}{d\tau_{i}^{\diamond}}\Big(\sum_{\ell = 1}^{L}\sum_{j< k:j,k\neq i}\widetilde{A}_{ijk}J_{ijk\ell}(\theta^{*})\Big),}
\end{array}
\]
where \(\tau_{i}^{\diamond}\) is short for \(\tau_{i}^{\diamond}(\theta^{*})\). Since each 3-way comparison is independent, it can be shown that the variance of \(J_{i}^{*}\) is
\[
\mathrm{Var}(J_{i}^{*}|\widetilde{\mathcal{G}}) = \frac{L}{d^{2}(\tau_{i}^{\diamond})^{2}}\sum_{j< k:j,k\neq i}\widetilde{A}_{ijk}e^{\theta_{i}^{*}}\Big(\frac{(e^{\theta_{j}^{*}} + e^{\theta_{k}^{*}})}{f^{2}(\{i,j,k\})}(e^{\theta_{j}^{*}} + e^{\theta_{k}^{*}} + e^{\theta_{i}^{*}}) + \frac{e^{\theta_{j}^{*}}e^{\theta_{k}^{*}}}{f^{2}(\{j,k\})}\Big).
\]
The essential component is to compute \(E[J_{ijkl}(\theta^{*})^{2}]\) due to independence and zero-mean. For a given triplet \((i,j,k)\), there are 6 possible preference outcomes with probabilities governed by the PL model. Averaging the squared random outcomes over 6 probabilities gives \(E[J_{ijkl}(\theta^{*})^{2}]\), which results in the expression above. We omit the details of these calculations.

Let us consider the simple situation that all \(\theta_{i}^{*}\) are equal. In this case, if we apply the most efficient weighting function \(f(A_{l})\propto \sum_{u\in A_{l}}e^{\theta_{u}^{*}}\), that is, \(f(\{i,j,k\}) = 3\), \(f(\{i,j\}) = f(\{i,k\}) = 2\), we have \(\mathrm{Var}(J_i^* |\widetilde{G}) = 18 / (7L)\). However, if we naively choose \(f\) as a constant function, we get \(\mathrm{Var}(J_i^* |\widetilde{G}) = 8 / (3L)\), which is indeed larger. It is also worth noting that when we choose \(f(A_{l})\propto \sum_{u\in A_{l}}e^{\theta_{u}^{*}}\), the aforementioned variance matches with the variance of MLE in Fan et al.~(2022b).

With the above formula of \(\mathrm{Var}(J_i^* |\widetilde{G})\) for the PL model, we can also conclude that
\[
\mathrm{Var}(J_i^* |\widetilde{G})^{-1 / 2}(\widetilde{\theta}_i - \theta_i^*)\Rightarrow N(0,1),
\]
for all \(i\in [n]\). The rigorous arguments will be introduced in Section 4.

\section{3.3 Ranking inference: one-sample confidence intervals}\label{ranking-inference-one-sample-confidence-intervals}

In numerous practical applications, individuals frequently interact with data and challenges related to rankings. The prevalent approach to utilizing rankings typically revolves around computing preference scores and then showcasing these scores in ranked order. These only provide first-order information on ranks and can not answer many questions, such as: How do we ascertain with high confidence that an item's true rank is among the top-3 (or general \(K,K\geq 1\)) choices? And how can we establish a set of candidates with high confidence, guaranteeing that the true top-3 candidates are not overlooked? How do we analyze if the ranking preferences for a given array of products are consistent in two distinct communities (such as male and female) or the same community but at two different time periods? In sum, there is a need for tools and methodologies that address these and other insightful queries in real-world applications involving rankings, especially when the comparisons are drawn from a general comparison graph.

Within this section, we first present a comprehensive framework designed for the construction of two-sided confidence intervals for ranks. In endeavoring to establish simultaneous confidence intervals for the ranks, an intuitive methodology entails deducing the asymptotic distribution of the empirical ranks, denoted as \(\widetilde{r}_{m},m\in \mathcal{M}\), and subsequently determining the critical value. However, it is well known that this task poses substantive challenges, given that \(\widetilde{r}_{m}\) is an integer and is intrinsically dependent on all estimated scores, making its asymptotic behavior daunting to analyze.

By capitalizing on the inherent interdependence between the scores and their corresponding ranks, we discern that the task of formulating confidence intervals for the ranks can be effectively converted to the construction of simultaneous confidence intervals for the pairwise differences among the population scores. It is notable that the distribution of these empirical score differences is more amenable to characterization. Consequently, we focus on the statistical properties of the estimated scores \(\widetilde{\theta}_{m} \in [n]\) and present our methodology for constructing two-sided (simultaneous) confidence intervals for ranks through estimated score differences.

Example 3.1. We let \(\mathcal{M} = \{m\}\), where \(1 \leq m \leq n\), to represent the item under consideration. We are interested in the construction of the \((1 - \alpha) \times 100\%\) confidence interval for the true population rank \(r_{m}\), where \(\alpha \in (0,1)\) denotes a pre-specified significance level. Suppose that we are able to construct the simultaneous confidence intervals \([\mathcal{C}_L(k,m), \mathcal{C}_U(k,m)], k \neq m, (k \in [n])\) for the pairwise differences \(\theta_{k}^{*} - \theta_{m}^{*}, k \neq m (k \in [n])\), with the following property:
\[
\mathbb{P}\Big(\mathcal{C}_L(k,m) \leq \theta_k^* - \theta_m^* \leq \mathcal{C}_U(k,m) \text{for all} k \neq m\Big) \geq 1 - \alpha . \tag{3.9}
\]
One observes that if \(\mathcal{C}_U(k,m) < 0\) (respectively, \(\mathcal{C}_L(k,m) > 0\)), it implies that \(\theta_k^* < \theta_m^*\) (respectively, \(\theta_k^* > \theta_m^*\)). Enumerating the number of items whose scores are higher than item \(m\) gives a lower bound for rank \(r_{m}\), and vice versa. In other words, we deduce from (3.9) that
\[
\mathbb{P}\left(1 + \sum_{k \neq m} 1 \{C_L(k,m) > 0\} \leq r_m \leq n - \sum_{k \neq m} 1 \{C_U(k,m) < 0\}\right) \geq 1 - \alpha . \tag{3.10}
\]
This yields a \((1 - \alpha) \times 100\%\) confidence interval for \(r_{m}\), and our task reduces to construct simultaneous confidence intervals for the pairwise differences (3.9).

We now formally introduce the procedure to construct the confidence intervals for multiple ranks \(\{r_m\}_{m \in \mathcal{M}}\) simultaneously. Motivated by Example 3.1, the key step is to construct the simultaneous confidence intervals for the pairwise score differences \(\{\theta_k^* - \theta_m^*\}_{m \in \mathcal{M}, k \neq m}\) such that (3.9) holds. To this end, we let
\[
T_{\mathcal{M}} = \max_{m \in \mathcal{M}} \max_{k \neq m} \left| \frac{\widetilde{\theta}_k - \widetilde{\theta}_m - (\theta_k^* - \theta_m^*)}{\widetilde{\sigma}_{km}} \right|, \tag{3.11}
\]
where \(\{\widetilde{\sigma}_{km}\}_{k \neq m}\) is a sequence of positive normalization given by (3.13) below. For any \(\alpha \in (0,1)\), let \(Q_{1 - \alpha}\) be critical value such that \(\mathbb{P}(T_{\mathcal{M}} \leq Q_{1 - \alpha}) \geq 1 - \alpha\). Then, as in Example 3.1, our \((1 - \alpha) \times 100\%\) simultaneous confidence intervals for \(\{r_m\}_{m \in \mathcal{M}}\) are given by \(\{[R_{mL}, R_{mU}] \}_{m \in \mathcal{M}}\), where
\[
R_{mL} = 1 + \sum_{k \neq m} 1 \left(\widetilde{\theta}_k - \widetilde{\theta}_m > \widetilde{\sigma}_{km} \times Q_{1 - \alpha}\right), \quad R_{mU} = n - \sum_{k \neq m} 1 \left(\widetilde{\theta}_k - \widetilde{\theta}_m < -\widetilde{\sigma}_{km} \times Q_{1 - \alpha}\right).
\]

\section{3.4 Multiplier bootstrap procedure}\label{multiplier-bootstrap-procedure}

The key step for constructing the confidence interval of ranks of interest is to pick the critical value \(Q_{1 - \alpha}\). To calculate the critical value above, we propose to use the wild bootstrap procedure. The uncertainty quantification for the spectral estimator in (3.2) reveals that \(\widetilde{\theta_{i}} - \theta_{i}^{*} \approx J_{i}(\theta^{*})\) uniformly over \(i \in [n]\) (see details in Section 4), which further implies that asymptotically
\[
T_{\mathcal{M}} \approx \max_{m \in \mathcal{M}} \max_{k \neq m} \left| \frac{J_{k}(\theta^{*}) - J_{m}(\theta^{*})}{\widetilde{\sigma}_{km}} \right|. \tag{3.12}
\]
We focus on the fixed graph setting and leave the random graph setting in Remark 3.2 below. Practically, the empirical version of \(J_{i}(\theta^{*})\) can be obtained via plugging in the spectral estimator \(\widetilde{\theta}\), namely from (3.4),
\[
J_{i}(\widetilde{\theta}) = \frac{1}{d} \sum_{l \in \mathcal{D}} J_{il}(\widetilde{\theta}), \quad i \in [n].
\]
Let \(\sigma_{km}^{2} = \operatorname {Var}\{J_{k}(\theta^{*}) - J_{m}(\theta^{*}) | \mathcal{G} \}\) for each \(k \neq m\). Then our estimator for \(\sigma_{km}^{2}\) is defined by
\[
\widetilde{\sigma}_{km}^{2} = \frac{e^{\widetilde{\theta}_{k}}}{d^{2} \tau_{k}^{2}(\widetilde{\theta})} \sum_{l \in \mathcal{D}} \frac{1(k \in A_{l})}{f^{2}(A_{l})} \left(\sum_{j \in A_{l}} e^{\widetilde{\theta}_{j}} - e^{\widetilde{\theta}_{k}}\right) \frac{e^{\widetilde{\theta}_{k}}}{\sum_{j \in A_{l}} e^{\widetilde{\theta}_{j}}} + \dots
\]
where \(\tau_{k}(\widetilde{\theta})\) and \(\tau_{m}(\widetilde{\theta})\) also plug in \(\widetilde{\theta}\); see (3.5). Let \(\omega_{1}, \ldots , \omega_{|\mathcal{D}|} \in \mathbb{R}\) be i.i.d. \(N(0,1)\) random variables. The Gaussian multiplier bootstrap statistic is then defined by
\[
G_{\mathcal{M}} = \max_{m \in \mathcal{M}} \max_{k \neq m} \left| \frac{1}{d \sigma_{km}} \sum_{l \in \mathcal{D}} \{J_{kl}(\widetilde{\theta}) - J_{ml}(\widetilde{\theta})\} \omega_{l} \right|. \tag{3.14}
\]
Let \(\mathbb{P}^{*}(\cdot) = \mathbb{P}(\cdot | \{(c_{i}, A_{i})\}_{i \in \mathcal{D}})\) denote the conditional probability. Then, for \(\alpha \in (0,1)\), our estimator for \(Q_{1 - \alpha}\) is defined by the \((1 - \alpha)\)th conditional quantile of \(G_{\mathcal{M}}\), namely
\[
\mathcal{Q}_{1 - \alpha} = \inf \{z: \mathbb{P}^{*}(G_{\mathcal{M}} \leq z) \geq 1 - \alpha \} ,
\]
which can be computed by the Monte Carlo simulation. Then, our simultaneous confidence intervals \(\{[\mathcal{R}_{mL}, \mathcal{R}_{mU}] \}_{m \in \mathcal{M}}\) are given by
\[
\mathcal{R}_{mL} = 1 + \sum_{k \neq m} 1 \left(\widetilde{\theta}_{k} - \widetilde{\theta}_{m} > \widetilde{\sigma}_{km} \times \mathcal{Q}_{1 - \alpha}\right), \qquad \mathcal{R}_{mU} = n - \sum_{k \neq m} 1 \left(\widetilde{\theta}_{k} - \widetilde{\theta}_{m} < -\widetilde{\sigma}_{km} \times \mathcal{Q}_{1 - \alpha}\right).
\]

Remark 3.1 (One-sample one-sided confidence intervals). Now we provide details on constructing simultaneous one-sided intervals for population ranks. For one-sided intervals, the overall procedure is similar to constructing two-sided confidence intervals. Specifically, let
\[
G_{\mathcal{M}}^{\circ} = \max_{m \in \mathcal{M}} \max_{k \neq m} \frac{1}{d \sigma_{km}} \sum_{l \in \mathcal{D}} \{J_{kl}(\widetilde{\theta}) - J_{ml}(\widetilde{\theta})\} \omega_{l}, \tag{3.16}
\]
where \(\omega_{1}, \ldots , \omega_{|\mathcal{D}|}\) are as before i.i.d. \(N(0,1)\) random variables. Correspondingly, let \(\mathcal{Q}_{1 - \alpha}^{\circ}\) be its \((1 - \alpha)\)th quantile. Then the \((1 - \alpha) \times 100\%\) simultaneous lower confidence bounds for \(\{r_{m}\}_{m \in \mathcal{M}}\) are given by \(\{[\mathcal{R}_{mL}^{ \circ }, n] \}_{m \in \mathcal{M}}\), where
\[
\mathcal{R}_{mL}^{\circ} = 1 + \sum_{k \neq m} 1 \left(\widetilde{\theta}_{k} - \widetilde{\theta}_{m} > \widetilde{\sigma}_{km} \times \mathcal{Q}_{1 - \alpha}^{\circ}\right). \tag{3.17}
\]

Remark 3.2 (Ranking inference for the PL model with random comparison graph). Section 3.2 reveals that \(\widetilde{\theta}_{i} - \theta_{i}^{*} \approx J_{i}(\theta^{*})\) uniformly over \(i \in [n]\), where following (3.8),
\[
J_{i}(\theta^{*}) = \frac{1}{d} \sum_{\ell = 1}^{L} \sum_{j < s:j, s \neq i} J_{ijs\ell}(\theta^{*}).
\]
In order to carry out ranking inference for the PL model, we need to rewrite this equation in a slightly different format. Let \(\begin{array}{r}{\mathcal{N} = \sum_{i< j< k}\widetilde{A}_{i j k}} \end{array}\) denote the total number of connected components on the random graph \(\widetilde{G}\) and write \(\{(i,j,k):i< j< k\) and \(\widetilde{A}_{i j k} = 1\} \eqqcolon \{\widetilde{A}_{q}\}_{q = 1,\ldots ,\mathcal{N}}\). Let \(y_{q}^{(\ell)}\) denote the \(\ell\)-th full-ranking comparison result for \(A_{q}\). Then we can rewrite \(P_{ij}\) as
\[
P_{ij} = \frac{1}{d} \sum_{\ell = 1}^{L} \sum_{q = 1}^{N} \sum_{k \neq i, j} 1\{(i,j,k) = \widetilde{A}_{q}\} Z_{ijkq}^{(\ell)}, i \neq j,
\]
where for \(i\neq j\neq k\) and \(q\in \lfloor \mathcal{N}\rfloor\), and \(Z_{i j k q}^{(\ell)} = 1\{y_{q}^{(\ell)} = (k\succ j \succ i)\} /f(\{i,j\}) + (1\{y_{q}^{(\ell)} = (j\succ i \succ k)\} /f(\{i,j,k\})) +1\{y_{q}^{(\ell)} = (j\succ k \succ i)\} /f(\{i,j,k\})\). It is straightforward to verify that this \(P_{ij}\) is exactly the same with (3.7). Therefore, we rewrite \(\begin{array}{r}{J_{i}(\theta^{*}) = d^{- 1}\sum_{\ell = 1}^{L}\sum_{q = 1}^{N}J_{i q\ell}^{\diamond}(\theta^{*})} \end{array}\), where
\[
J_{iq\ell}^{\diamond}(\theta^{*}) = \sum_{j < s:j, s \neq i} 1\{(i,j,s) = \widetilde{A}_{q}\} J_{iq\ell}^{\diamond}(\theta^{*}). \tag{3.18}
\]
As is assumed, \(\{J_{iq\ell}^{\diamond}(\theta^{*})\}_{\ell \in [L], q \in [\mathcal{N}]}\) are independent for each \(i \in [n]\) conditioning on the comparison graph \(\widetilde{G}\). Let \(\{\omega_{q\ell}^{\diamond}\}_{q, \ell \in \mathbb{N}}\) be i.i.d. \(N(0,1)\) random variables. Then, following (3.14), the corresponding bootstrap test statistic is given by
\[
G_{\mathcal{M}}^{\diamond} = \max_{m \in \mathcal{M}} \max_{k \neq m} \left| \frac{1}{d \sigma_{km}^{\diamond}} \sum_{\ell = 1}^{L} \sum_{q = 1}^{N} \{J_{kq\ell}^{\diamond}(\widetilde{\theta}) - J_{mq\ell}^{\diamond}(\widetilde{\theta})\} \omega_{q\ell}^{\diamond} \right|,
\]
where \(\{\widetilde{\sigma}_{km}^{\diamond}\}_{k \neq m}\) are as before the sequence of positive normalization, calculated as the sum of the variance of \(J_{k}(\theta^{*})\) and \(J_{m}(\theta^{*})\) similar to (3.13). Consequently, the simultaneous confidence intervals for the ranks can be similarly constructed.

\section{3.5 Ranking inference: two-sample and one-sample testing applications}\label{ranking-inference-two-sample-and-one-sample-testing-applications}

In this section, we further illustrate how we may apply our inference methodology to a few salient testing applications, in both one-sample and two-sample testing.

Example 3.2 (Testing top-\(K\) placement). Let \(\mathcal{M} = \{m\}\) for some \(m \in [n]\) and let \(K \geq 1\) be a prescribed positive integer. Our objective is to ascertain if the item \(m\) is a member of the top-\(K\) ranked items. Consequently, we shall examine the following hypotheses:
\[
H_{0}:r_{m}\leq K\mathrm{~versus~}H_{1}:r_{m} > K. \tag{3.19}
\]
Based on the one-sided confidence interval \([\mathcal{R}_{mL}^{\circ}, n]\) in (3.17), for any \(\alpha \in (0,1)\), a level \(\alpha\) test for (3.19) is simply given by \(\phi_{m,K} = 1\{\mathcal{R}_{mL}^{\circ} > K\}\). Under the conditions of Theorem E.1, we have \(\mathbb{P}(\phi_{m,K} = 1|H_{0}) \leq \alpha + \mathrm{o}(1)\), that is, the effective control of the Type-I error can be achieved below the significant level \(\alpha\) when the null hypothesis is true.

Example 3.3 (Top-\(K\) sure screening set). Another example is on constructing a screened candidate set that contains the top-\(K\) items with high probability. This is particularly useful in college candidate admission or company hiring decisions. Oftentimes, a university or a company would like to design certain admission or hiring policy with the high-probability guarantee of the sure screening of true top-\(K\) candidates.
Let \(\mathcal{K} = \{r^{- 1}(1),\ldots ,r^{- 1}(K)\}\) denote the top-\(K\) ranked items of the rank operator \(r:[n]\to [n]\). We aim at selecting a set of candidates \(\widehat{\mathcal{I}}_K\) which contains the top-\(K\) candidates with a prescribed probability. Mathematically, this requirement can be expressed as \(\mathbb{P}(\mathcal{K}\subseteq \widehat{\mathcal{I}}_K)\geq 1 - \alpha\), where \(\alpha \in (0,1)\). Herein, we define \(\mathcal{M} = [n]\), and let \(\{[\mathcal{R}_{mL}^{\circ},n],m\in [n]\}\) represent the set of \((1 - \alpha)\times 100\%\) simultaneous left-sided confidence intervals, as given in (3.17). It is easy to observe that the inequality \(\mathcal{R}_{mL}^{\circ} > K\) infers that \(r_m > K\). Consequently, a selection for \(\widehat{\mathcal{I}}_K\), that satisfies the probability constraint \(\mathbb{P}(\mathcal{K}\subseteq \widehat{\mathcal{I}}_K)\geq 1 - \alpha\), is given by
\[
\widehat{\mathcal{I}}_K = \{m\in [n]:\mathcal{R}_{mL}^{\circ}\leq K\} .
\]

Example 3.4 (Testing ranks of two samples). In many applications, we are concerned with the question of whether the ranks of certain items using two samples have been changed or preserved. For example, we may care about whether
\begin{itemize}
\tightlist
\item
  Ranking allocation differs before and after a treatment or policy change.
\item
  Different communities such as males versus females can have different ranking preferences over the same set of products.
\item
  People's perceived preferences over the same things have changed in two time periods.
\end{itemize}
Suppose we observe two independent datasets \(\mathcal{D}_1\) and \(\mathcal{D}_2\) with preference scores \(\theta_{[1]}^{*} = (\theta_{11}^{*},\ldots ,\theta_{1n}^{*})^{\top}\) and \(\theta_{[2]}^{*} = (\theta_{21}^{*},\ldots ,\theta_{2n}^{*})^{\top}\). The associated true rankings are respectively denoted by
\[
r_{[1]} = (r_{11},\ldots ,r_{1n})^{\top} \text{and} r_{[2]} = (r_{21},\ldots ,r_{2n})^{\top}.
\]
Given any \(m\in [n]\), we are interested in testing whether the same rank is preserved for item \(m\) across these two samples, that is, testing the hypotheses
\[
H_{0}:r_{1m} = r_{2m} \text{versus} H_{1}:r_{1m}\neq r_{2m} \tag{3.20}
\]
To this end, firstly we construct simultaneous confidence intervals \([R_{1mL},R_{1mU}]\) and \([R_{2mL},R_{2mU}]\) such that
\[
\mathbb{P}(r_{1m}\in [R_{1mL},R_{1mU}]\text{and} r_{2m}\in [R_{2mL},R_{2mU}])\geq 1 - \alpha . \tag{3.21}
\]
Then our \(\alpha\)-level test for (3.20) is defined by
\[
\phi_{m} = 1\{|[R_{1mL},R_{1mU}]\cap [R_{2mL},R_{2mU}]| = 0\} .
\]
It is straightforward to verify that \(\mathbb{P}(\phi_{m} = 1|H_{0})\geq 1 - \alpha\).

Example 3.5 (Testing top-\(K\) sets of two samples). Besides testing for a single or a few ranks, one may want to evaluate whether two top-\(K\) sets are identical or not, between two groups of people, two periods of time, or before and after a significant event or change. Let \(\mathcal{S}_{1K} = \{r_{[1]}^{- 1}(1),\ldots ,r_{[1]}^{- 1}(K)\}\) and \(\mathcal{S}_{2K} = \{r_{[2]}^{- 1}(1),\ldots ,r_{[2]}^{- 1}(K)\}\) denote the sets of top-\(K\) ranked items, respectively. We consider testing the hypotheses
\[
H_{0}:S_{1K} = S_{2K}\mathrm{~versus~}H_{1}:S_{1K}\neq S_{2K}. \tag{3.22}
\]
For \(\alpha \in (0,1)\), we begin with constructing \((1 - \alpha)\times 100\%\) simultaneous confidence sets \(\widehat{\mathcal{I}}_{1K}\) and \(\widehat{\mathcal{I}}_{2K}\) for \(S_{1K}\) and \(S_{2K}\) such that
\[
\mathbb{P}\left(S_{1K}\subset \widehat{\mathcal{I}}_{1K}\mathrm{~and~}S_{2K}\subset \widehat{\mathcal{I}}_{2K}\right)\geq 1 - \alpha . \tag{3.23}
\]
Then our \(\alpha\)-level test for (3.22) is defined by
\[
\widetilde{\phi}_{K} = 1\{|\widehat{\mathcal{I}}_{1K}\cap \widehat{\mathcal{I}}_{2K}|< K\} .
\]

Remark 3.3. Several methodologies, including the Bonferroni adjustment (which constructs a \((1 - \alpha /2)\times 100\%\) confidence interval for each source), and Gaussian approximation (achieved by taking the maximum of the test statistics of each source), enable us to establish simultaneous confidence intervals as illustrated in equations (3.21) and (3.23). To maintain clarity and simplicity in the subsequent context, we simply employ the Bonferroni adjustment for two samples. Moreover, the framework outlined in Examples 3.4 and 3.5 can be extended in a straightforward way to evaluate whether the ranks of items or sets are identical across three or more sources.

\section{4 Theoretical Justifications}\label{theoretical-justifications}

In this section, we rigorously justify the conclusions in Section 3 and explicitly lay out the necessary assumptions to arrive at those conclusions. The first assumption is to make sure we are comparing \(\theta_{i}^{*}\)'s in the same order in a meaningful way. Otherwise, we can always group items into categories with similar qualities and then work on each sub-group or screen some extreme items. In addition, as we have discussed, we need an identifiability condition for \(\theta^{*}\).

Assumption 4.1. There exists some positive constant \(\bar{\kappa} < \infty\) such that
\[
\max_{i\in [n]}\theta_{i}^{*} - \min_{i\in [n]}\theta_{i}^{*}\leq \bar{\kappa}.
\]
In addition, for identifiability, assume \(1^{\top}\theta^{*} = 0\).

In Assumption 4.1, we assume \(\bar{\kappa}\) is finite, indicating we only rank items with preference scores on the same scale. If \(\bar{\kappa}\) is diverging, some items will be trivially more or less favorable than others. In this case, it is typically easy in practice to separate the items into subgroups with similar preference scores, and then we can conduct ranking inference within each group. Although we assume bounded \(\bar{\kappa}\), it serves as the role of a condition number whose effect has been made explicit in all our results for interested readers. However, we do not claim this dependency is optimal as our nontrivial analysis can easily encounter powers of \(e^{\bar{\kappa}}\), say in bounding the ratio of \(\pi_{i}^{*} / \pi_{j}^{*}\).

\section{4.1 Estimation accuracy and asymptotic normality with fixed comparisons}\label{estimation-accuracy-and-asymptotic-normality-with-fixed-comparisons}

To derive the asymptotic distribution of the spectral estimator, we need to rigorously justify the approximations (3.1) and (3.2). We first take care of approximating (3.1) using (3.2), where all comparisons in \(\mathcal{G}\) are assumed to be fixed. Note that
\[
P_{ij} - E[P_{ij}|\mathcal{G}] = \frac{1}{d}\sum_{l\in \mathcal{D}}1(i,j\in A_l)\left[1(c_l = j) - \frac{\pi_j^*}{\sum_{u\in A_l}\pi_u^*}\right]\frac{1}{f(A_l)}.
\]
Let \(Z_{A_l}^j = 1(c_l = j) / f(A_l)\), which is bounded from above and below as long as \(f\) is bounded from above and below. Furthermore, each \(Z_{A_l}^j\) is independent. Therefore, \(P_{ij} - E[P_{ij}|\mathcal{G}] = d^{- 1}\sum_{l\in \mathcal{D}}1(i,j\in A_l)[Z_{A_l}^j - E(Z_{A_l}^j)]\). By Hoeffding's inequality, conditioning on \(\mathcal{G}\), we have with a large probability \(1 - o(1)\),
\[
\max_{i\neq j}\left|P_{ij} - E[P_{ij}|\mathcal{G}]\right|\lesssim \frac{1}{d}\sqrt{(\log n)n^{\ddagger}}.
\]
where \(\begin{array}{r}{n^{\ddagger} = \max_{i\neq j}\sum_{l\in \mathcal{D}}1(i,j\in A_{l})} \end{array}\) is the maximum number of cases that each pair is compared. Similarly, we can get the concentration bound for \(\sum_{j:j\neq i}P_{ij}\). Since \(Z_{A_{l}}^{j}\)'s are independent, another level of summation over \(j\) will lead to the following. Again by Hoeffding's inequality, with a large probability tending to 1, we obtain
\[
\max_{i}\left|\sum_{j:j\neq i}P_{ij} - \sum_{j:j\neq i}E[P_{ij}|\mathcal{G}]\right|\lesssim \frac{1}{d}\sqrt{(\log n)n^{\dagger}},
\]
where \(\begin{array}{r}{n^{\dagger} = \max_{i}\sum_{l\in \mathcal{D}}1(i\in A_{l})} \end{array}\) is the maximum number of cases that each item is compared. In addition, we assume
\[
\sum_{j:j\neq i}E[P_{ij}|\mathcal{G}] = \tau_{i}e^{-\theta_{i}^{*}}\asymp \frac{1}{d} n^{\dagger},
\]
where \(\tau_{i}\) defined in (3.3) is the denominator of \(J_{i}^{*}\). This assumption makes sense as \(\tau_{i}e^{- \theta_{i}^{*}}\lesssim \sum_{l\in \mathcal{D}}1(i\in A_{l}) / d\), and it states for each \(i\) the comparison graph cannot be too asymmetric. Note that \(\sum_{l\in \mathcal{D}}1(i,j\in A_{l})\) can still be widely different from \(n^{\ddagger}\) for different pair \((i,j)\). Since the expectation term dominates the deviation if \(n^{\dagger}\gtrsim \log n\), it is not hard to show that in (3.2), changing the denominator by its expectation will only cause a small order difference, which does not affect the asymptotic distribution.

Based on the above discussion, we impose the following assumption.

Assumption 4.2. In the case of a fixed comparison graph, we assume the graph is connected, \(\tau_{i}e^{- \theta_{i}^{*}}\asymp n^{\dagger} / d\) for all \(i\in [n]\), \(e^{2\bar{\kappa}}\log n = o(n)\) and \(e^{3\bar{\kappa}}n^{\ddagger}n^{1 / 2}(\log n)^{1 / 2} = o(n^{\dagger})\).

The assumption is reasonable for a fixed comparison graph. If each pair \((i,j)\) must be compared at least once, then every \(\sum_{l\in \mathcal{D}}1(i,j\in A_{l})\geq 1\). If they are all in the same order, then \(\sum_{l\in \mathcal{D}}1(i\in A_{l}) = \sum_{j:j\neq i}\sum_{l\in \mathcal{D}}1(i,j\in A_{l})\) should be indeed in the order of \(n^{\ddagger}n\). Assumption 4.2 allows some pair \((i,j)\) to be never compared directly, so we need to leverage the information from comparing \(i\) and \(j\) to other items separately. Moreover, we also do not require \(\sum_{l\in \mathcal{D}}1(i,j\in A_l)\) to be in the same order for any \(i,j:i\neq j\) since we only require the maximum pairwise degree \(n^{\ddagger}\) to satisfy Assumption 4.2. However, in the case of a fixed graph, we do not have the randomness from the graph, and the graph must be relatively dense to make sure we have enough information to rank every item. This condition will be relaxed to \(n^{\dagger}\gtrsim n^{\ddagger}\log n\) when we have a homogeneous random comparison graph in Section 4.2.

We need another technical condition on the structure of the comparison graph. Define \(\Omega = \{\Omega_{ij}\}_{i\leq n,j\leq n}\) where \(\Omega_{ij} = - P_{ji}\pi_j^*\) for \(i\neq j\) and \(\Omega_{ii} = \sum_{j:j\neq i}P_{ij}\pi_i^*\). Note that as we derived above, \(E[\Omega_{ii}|\mathcal{G}]\) is in the order of \(n^{\dagger} / (dn)\). We hope to understand the order of its eigenvalues. Since \(\Omega\) has the minimal eigenvalue equal to zero, with the corresponding eigenvector \(\mathbf{1}\), we only focus on the space orthogonal to \(\mathbf{1}\). Following the notation of Gao et al.~(2021),
\[
\lambda_{\min ,\bot}(A) = \min_{\| v\| = 1,v^{\top}\mathbf{1} = 0}v^{\top}A v.
\]
Assumption 4.3. There exist \(C_1,C_2 > 0\) such that
\[
C_1e^{-\bar{\kappa}}\frac{n^\dagger}{dn}\leq \lambda_{\min ,\bot}(E[\Omega |\mathcal{G}])\leq \lambda_{\max}(E[\Omega |\mathcal{G}])\leq C_2e^{\bar{\kappa}}\frac{n^\dagger}{dn}, \tag{4.1}
\]
\[
\| \Omega -E[\Omega |\mathcal{G}]\| = o_{P}\left(\frac{n^{\dagger}}{dn}\right). \tag{4.2}
\]
When \(\bar{\kappa} = O(1)\), Assumption 4.3 requires that all eigenvalues (except the minimal one) of \(E[\Omega |\mathcal{G}]\) are in the order of \(n^{\dagger} / (dn)\) and \(\Omega\) also shares this same eigenvalue scale as \(E[\Omega |\mathcal{G}]\). This assumption is intuitively correct, as we have seen that \(E[\Omega_{ij}]\lesssim n^{\ddagger} / (dn)\) for \(i\neq j\) and \(E[\Omega_{ii}]\asymp n^{\dagger} / (dn)\). We will also rigorously show that this condition can be satisfied if we consider the PL model (Theorem 4.3).

Theorem 4.1. Under Assumptions 4.1-4.3, the spectral estimator \(\widetilde{\theta_{i}}\) has the following uniform approximation: \(\widetilde{\theta}_{i} - \theta_{i}^{*} = J_{i}^{*} + \delta_{i}\), uniformly for all \(i\in [n]\), where \(\| \delta \coloneqq (\delta_{1},\dots ,\delta_{n})\|_{\infty} = o(1 / \sqrt{n^{\dagger}})\) with probability \(1 - o(1)\).

To prove Theorem 4.1, we need to verify (3.1). We leave the detailed proof in the appendix. Given Theorem 4.1, we can easily conclude the next theorem following the properties of \(J_{i}^{*}\), which lead to the rate of convergence for \(\widetilde{\theta}\) as well as its asymptotic normality.

Remark 4.1. The results of Theorem 4.1 and the following Theorems are proved via Bernstein and Hoeffding type inequalities with union bound over \(n\) items. Therefore, all of the high-probability terms hold with probability \((1 - o(1))\) (similarly for \(o_{p}(\cdot)\) and \(O_{p}(\cdot)\)) mentioned in the main text equivalently hold with probability in form of \(1 - O(n^{- \zeta})\) where \(\zeta \geq 2\) is a positive integer (different choice of \(\zeta\) will only affect constant terms in the involved concentration inequalities).

Theorem 4.2. Under Assumptions 4.1-4.3, the spectral estimator (2.1) satisfies that
\[
\| \widetilde{\theta} -\theta^{*}\|_{\infty}\asymp \| J^{*}\|_{\infty}\lesssim e^{\bar{\kappa}}\sqrt{\frac{\log n}{n^{\dagger}}}, \tag{4.3}
\]
with probability \(1 - o(1)\), where \(J^{*} = (J_{1}^{*},\dots ,J_{n}^{*})\) with \(J_{i}^{*},i\in [n]\) being defined in (3.4). In addition,
\[
\rho_{i}(\theta)(\widetilde{\theta}_{i} - \theta_{i}^{*})\Rightarrow N(0,1),
\]
for all \(i\in [n]\) with
\[
\rho_{i}(\theta) = \left[\sum_{l\in \mathcal{D}}1(i\in A_{l})\left(\frac{\sum_{u\in A_{l}}e^{\theta_{u}} - e^{\theta_{i}}}{\sum_{u\in A_{l}}e^{\theta_{u}}}\right)\frac{e^{\theta_{i}}}{f^{2}(A_{l})}\right]^{-1/2},
\]
for both \(\theta = \theta^{*}\) and \(\theta =\) any consistent estimator of \(\theta^{*}\).

Note that Theorem 4.2 indicates that the choice of \(f(\cdot) > 0\) does not affect the rate of convergence, but it affects the estimation efficiency. As we argued in Section 3.1, the optimal weighting to minimize the asymptotic variance is \(f(A_{l})\propto \sum_{u\in A_{l}}e^{\theta_{u}^{*}}\) in the class of spectral estimators. In practice, however, we do not know \(\theta_{u}^{*}\) beforehand. Therefore, we could implement a two-step procedure to improve the efficiency of the spectral estimator: in the first step, we obtain our initial consistent estimator \(\widehat{\theta_{u}^{(\mathrm{initial})}}\) with weighting say \(f(A_{l}) = |A_{l}|\), and in the second step, we estimate \(f(A_{l}) = \sum_{u\in A_{l}}e^{\theta_{u}^{*}}\) by plugging \(\widehat{\theta_{u}^{(\mathrm{initial})}}\) and run the spectral method again with this optimal weighting to get the final asymptotically efficient estimator \(\widehat{\theta_{u}^{(\mathrm{final})}}\). Note that we do not intend to prove the theoretical properties of this two-step estimator, as the data dependency in the optional weighting of the second step makes the uniform approximation analysis highly nontrivial due to non-i.i.d. ranking outcomes. Nonetheless, we could circumvent this theoretical difficulty by splitting data into a very small part \((o(|\mathcal{D}|)\) samples) for step 1, to achieve consistency with a worse convergence rate, and using the remaining majority \((|\mathcal{D}| - o(|\mathcal{D}|)\) samples) for step 2, to maintain the same asymptotic behavior. In addition, empirically, we found that directly using the same whole data in both steps achieves decent performance given a large sample size. We refer interested readers to our numerical studies.

\section{4.2 Estimation accuracy and asymptotic normality for the PL model}\label{estimation-accuracy-and-asymptotic-normality-for-the-pl-model}

In the random graph case, we have to specify the graph generation process in order to study the theoretical properties. We consider the commonly used PL model, where we sample each \(M\)-way comparison with probability \(p\) and compare this set for \(L\) times. Furthermore, we will only work with \(M = 3\) since we plan to focus on a transparent and intuitive discussion. We can easily generalize all the discussions to general \(M\), but derivations and formulas can be more tedious.

The PL model with 3-way comparisons has been studied in Fan et al.~(2022b) by using MLE, where they explicitly write down the likelihood function. The proposed spectral method can work for any fixed graph, including the one generated from the PL model. In this section, we would like to compare the performance of the spectral method with that of the MLE. To make sure the spectral method works for the PL model, we need to prove the approximations (3.1) and (3.6).

We first take care of (3.6). Consider conditioning on \(\widetilde{\mathcal{G}}\), where all comparisons in \(\widetilde{\mathcal{G}}\) are independent; each \(\widetilde{A}_{ijk}\) is compared for \(L\) times if \(\widetilde{A}_{ijk} = 1\). Now \(c_{l}\) and \(A_{l}\) are induced from \(\widetilde{\mathcal{G}}\), and can be dependent. In this case, we can write
\[
P_{ij} - E[P_{ij}|\widetilde{\mathcal{G}} ] = \frac{1}{d}\sum_{\ell = 1}^{L}\sum_{k:k\neq j,i}\widetilde{A}_{ijk}[Z_{ijk}^{l} - EZ_{ijk}^{l}],
\]
where \(Z_{ijk}^{l} = 1(A_{l} = \{i,j\} ,c_{l} = j) / f(\{i,j\}) + 1(A_{l} = \{i,j,k\} ,c_{l} = j) / f(\{i,j,k\})\), which is again bounded from above and below and independent for any given \(\widetilde{A}_{ijk}\). In this case, with a little abuse of notations, we redefine
\[
n^{\dagger} = L\max_{i\neq j}\sum_{k:k\neq j,i}A_{ijk},\quad n^{\dagger} = L\max_{i}\sum_{j< k:j,k\neq i}A_{ijk}.
\]
Similar to Section 4.1, conditional on \(\widetilde{G}\), we have
\[
\begin{array}{r l} & {\max_{i}\bigg|\sum_{j:j\neq i}P_{i j} - \sum_{j:j\neq i}E[P_{i j}|\widetilde{\mathcal{G}} ]\bigg| = \mathcal{O}_{P}(d^{-1}\sqrt{n^{\dagger}\log n}),}\\ & {\max_{i\neq j}\bigg|P_{ij} - E[P_{ij}|\widetilde{\mathcal{G}}]\bigg| = \mathcal{O}_{P}(d^{-1}\sqrt{n^{\ddagger}\log n}).}
\end{array}
\]
We adapt Assumption 4.2 to the following assumption. Note that we have no assumption on \(L\), so \(L\) can be as low as 1.

Assumption 4.4. In the PL model with \(M\)-way complete comparisons, choose \(d\asymp n^{\dagger}\) in the spectral ranking, and assume \(\tau_{i}^{\diamond}e^{- \theta_{i}^{*}}\asymp n^{\dagger} / d\) for all \(i\in [n]\), \(e^{4\bar{\kappa}} = o(n)\) and \(p\gtrsim e^{6\bar{\kappa}}\mathrm{poly}(\log n) / \binom{n- 1}{M- 1}\).

Under Assumption 4.4, we can prove
\[
n^{\dagger}\asymp \binom{n-1}{M-1}p L,\qquad\max \Big\{\binom{n-2}{M-2}p-\log n,0\Big\}L\lesssim n^{\ddagger}\lesssim\Big[\binom{n-2}{M-2}p+\log n\Big]L,
\]
with probability \(1 - o(1)\). Note that in \(n^{\dagger}\), by Assumption 4.4, we know the dominating term is \(\binom{n- 1}{M- 1}p L\). However, in \(n^{\ddagger}\), we have the additional term \(\log n\), which comes from the sub-exponential tail decay in Bernstein inequality, and if \(p\) is really small, it could happen that \(\log n\) dominates \(n^{\ddagger}\). When \(p\) is large, that is \(\binom{n- 2}{M- 2}p\gtrsim\log n\), then \(n^{\dagger}\asymp n n^{\ddagger}\) and Assumption 4.2 holds. Therefore, we have a dense comparison graph, and the proof for this part follows in a similar vein as Theorem 4.2. When \(p\) is small, that is, \(\binom{n- 2}{M- 2}p\lesssim\log n\), \(n^{\ddagger}\lesssim\log n\) if \(L\) is bounded. In this case, we will modify the proof of Theorem 4.2 to the random graph case in order to show Theorem 4.4 below. In addition, since \(\sum_{j:j\neq i}P_{i j} = \mathcal{O}_{P}(n^{\dagger} / d)\), it makes sense to choose \(d\asymp n^{\dagger}\) in Assumption 4.4 to make the diagonal elements of the transition matrix a constant order. Note that in the fixed graph case, we do not need to impose rate assumptions on \(d\) as the comparison graph has no randomness.

Next, we verify that under the PL model, Assumption 4.3 holds with high probability.

Theorem 4.3. Under the PL model and Assumption 4.4, with probability \(1 - o(1)\), Assumption 4.3 holds when we condition on \(\widetilde{G}\) instead of \(\mathcal{G}\).

We next hope to show that under Assumptions 4.4, the spectral estimator \(\widehat{\theta}_{i}\) has the uniform approximation: the differences between \(\widetilde{\theta}_{i} - \theta_{i}^{*}\) and \(J_{i}^{*}\) for all \(i\in [n]\) are \(o_{P}(1 / \sqrt{n^{\dagger}})\). The key step is still the verification of (3.1) under this weaker Assumptions 4.4 for a random comparison graph.

Theorem 4.4. Under the PL model and Assumptions 4.1 and 4.4, the spectral estimator \(\widetilde{\theta}_{i}\) has the uniform approximation: \(\widetilde{\theta}_{i} - \theta_{i}^{*} = J_{i}^{*} + o_{P}\big(1 / \sqrt{n^{\dagger}}\big)\), uniformly for all \(i\in [n]\). Therefore, the spectral estimator (2.1) satisfies
\[
\| \widetilde{\theta} -\theta^{*}\|_{\infty}\lesssim e^{\bar{\kappa}}\sqrt{\frac{\log n}{\binom{n - 1}{M - 1}pL}}, \tag{4.5}
\]
with probability \(1 - o(1)\). In addition,
\[
\rho_{i}(\theta)(\widetilde{\theta}_{i} - \theta_{i}^{*})\Rightarrow N(0,1),
\]
for all \(i\in [n]\) with \(\rho_{i}(\theta) = \mathrm{Var}(J_{i}^{*}|\widetilde{G})^{- 1 / 2}\), where in the formula of \(\mathrm{Var}(J_{i}^{*}|\widetilde{G})\) we can choose both \(\theta = \theta^{*}\) and \(\theta =\) any consistent estimator of \(\theta^{*}\).

Remark 4.2. The two-step estimator under optimal weight \(f(A_{l}) = \sum_{u\in A_{l}}e^{\theta_{u}^{*}}\) (can be consistently estimated with a small proportion of a separate dataset) achieves the same variance as the MLE estimator, which matches the Cramer Rao lower bound among all estimators (Fan et al., 2022a,b).

Corollary 4.1. Under the conditions of Theorem 4.4, if we have \(\theta_{(K)}^{*} - \theta_{(K + 1)}^{*}\geq \Delta\), with \(\theta_{(i)}^{*}\) denoting the underlying score of the item with true rank \(i\) for \(i\in [n]\), and when the sample complexity satisfies
\[
e^{2\bar{\kappa}}\Delta^{-2}\cdot \log n = \mathcal{O}\bigg(\binom{n-1}{M-1}pL\bigg),
\]
we have \(\{i\in [n],\widehat{r_{i}}\leq K\} = \{i\in [n],r_{i}^{*}\leq K\}\) (the selected top-K set is identical to the true top-K set), where \(\widehat{r_{i}},r_{i}^{*}\) denote the empirical rank of \(\widehat{\theta}_{i}\) among \(\{\widehat{\theta}_{i},i\in [n]\}\) and true rank of the i-th item, respectively.

We remark that when \(M = 2\), and \(\bar{\kappa} = \mathcal{O}(1)\), our conclusion from Corollary 4.1 reduces to the conclusion of Theorem 1 in Chen et al.~(2019).

\section{4.3 Validity Justification for Bootstrap Procedure}\label{validity-justification-for-bootstrap-procedure}

The primary goal of this section is to justify the validity of the proposed bootstrap procedure in Section 3.4. Recall that the targeted quantity \(T_{\mathcal{M}}\) is the maximum modulus of the random vector
\[
\Delta_{\mathcal{M}}\coloneqq \left\{\frac{\widetilde{\theta}_{k} - \widetilde{\theta}_{m} - (\theta_{k}^{*} - \theta_{m}^{*})}{\widetilde{\sigma}_{km}}\right\}_{m\in \mathcal{M},k\neq m}.
\]
For each marginal of \(\Delta_{\mathcal{M}}\), the asymptotic normality can be similarly established following Theorem 4.2. However, studying the joint distribution is more complex.




\end{document}
